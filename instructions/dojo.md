summary: dojo - intÃ©gration continue 
id: dojo 
categories: exercice 
tags: exercice 
status: Published 
authors: MHO
Feedback Link: https://gitlab.com/skool-data-fy22/dojo-3/-/issues/new

# Pipelines de CI

## Overview

Duration: 30

Objectifs pÃ©dagogique :

- DÃ©couvrir les concepts autour de l'intÃ©gration continue,
- Programmer un pipeline d'intÃ©gration continue/une usine de dÃ©veloppement logiciel,
- Obtenir du feedback frÃ©quemment grÃ¢ce aux tests automatisÃ©s,
- Mesurer la qualitÃ© de son code,
- Automatiser la production d'artÃ©facts,
- Manipuler des registry d'artÃ©facts.

## Les concepts de l'intÃ©gration continue

Duration: 5

### Quelques slides pour dÃ©marrer

ğŸ‘‰ [Dispo sur le drive OCTO](https://docs.google.com/presentation/d/1Sgs9ZuW2vG0EWFqxGvPRgzpNHMqoFk9-5da3MnIA8qQ/edit#slide=id.g804284dca3_0_176)
.

### Ce qu'il faut retenir

- Comme disait Martin Fowler sur son blog :

```text
Continuous Integration is a software development practice where members of a team 
integrate their work frequently, usually each person integrates at least daily, 
leading to multiple integrations per day.
```

Pour permettre cela et produire du code de qualitÃ©, il faut de l'automatisation et de l'outillage ğŸ› 

Cet outillage, c'est gÃ©nÃ©ralement un pipeline, et on va en construire un pendant ce dojo ğŸ’ƒ

![](./docs/pipeline.png)

Pour les curieux qui veulent aller + loin sur la notion d'intÃ©gration continue :

- <https://www.martinfowler.com/articles/continuousIntegration.html>
- <https://blog.octo.com/tag/continuous-integration/>

## TP 1 - Tests automatisÃ©s (local)

**ğŸ¯ Objectif** : je veux obtenir du feedback sur le produit que je dÃ©veloppe via les tests

### Tests en local

```plaintext
> ğŸ•µï¸â€ Automatiser, c'est rendre automatique une action qui Ã©tait jusque-lÃ  manuelle ğŸ’ª
```

**ğŸ‘‰ Commencez par lancer les tests en local !** 

Au passage, prenez en note dans un fichier :
- les prÃ©-requis : les commandes ou paquets que vous avez dÃ» installer pour pouvoir lancer les tests,
- la commande que vous avez exÃ©cuter pour lancer les tests,
- le rÃ©sultat attendu : logs affichÃ©s en console, fichiers de rapport produits, ...

ğŸ Exemple de rÃ©sultat attendu en lanÃ§ant les tests en local avec 

```shell
$ PYTHONPATH=. pytest;
```

![](./docs/exercice1-exec-tests-local.png)

## TP 2 - Tests automatisÃ©s (CI)

**ğŸ¯ Objectif** : je veux obtenir du feedback sur mes tests Ã  chaque commit poussÃ© sur ma branche de travail.

**Rendu attendu Ã  la fin de ce TP2** : en poussant du code sur ma branche de travail, un pipeline doit se lancer automatiquement sur gitlab. Ce pipeline doit permettre d'exÃ©cuter les tests avec pytest, comme ceci quand les tests sont au vert :

![](./docs/exercice1-tests.png)

### Tests dans le pipeline de CI

ğŸ‘‰ Editez le fichier [.gitlab-ci.yml](../.gitlab-ci.yml) afin d'y ajouter un stage nommÃ© `python-tests`.

Ce stage contiendra 1 step nommÃ©e `Tests` qui doit exÃ©cuter les tests python rÃ©digÃ©s avec `pytest`.

|                   a                    |                   b                   |
|:--------------------------------------:|:-------------------------------------:|
| âœ… SuccÃ¨s si tous les tests sont verts  | ğŸ”´ Echec si au moins 1 test est rouge |
|    ![](./docs/exercice1-tests.png)     | ![](./docs/exercice1-tests-rouge.png) |

**ğŸ Test de recette** : Si la step `python-tests` s'exÃ©cute bien dans votre pipeline de CI,
- elle devrait arborer une coche verte, 
  - ![](./docs/exercice1-tests.png) 
- et afficher les logs d'exÃ©cution de la commande pytest en console.
  - ![](./docs/exercice1-logs-tests-en-ci.png)

â„¹ï¸ Si vous ne savez pas comment faire Ã©diter le pipeline, la partie ci-aprÃ¨s vous donnera un premier vernis sur les pipelines gitlab-ci et leur dÃ©claration en YAML.

#### ğŸ¦Š Un mot sur les pipelines Gitlab si vous n'avez jamais manipulÃ© cet outil

`Gitlab CI/CD` est un outil mis Ã  disposition par Gitlab pour construire des pipelines de traitement.

Ces pipelines peuvent Ãªtre utilisÃ©s Ã  des fins d'intÃ©gration continue.

Le pipeline est dÃ©crit au travers de code, dans un fichier [.gitlab-ci.yml](../../.gitlab-ci.yml), Ã  la racine du repo en langage [`YAML`](https://learnxinyminutes.com/docs/fr-fr/yaml-fr/), une spec de configuration similaire au `JSON`.

La documentation de gitlab ainsi que les mot-clefs utilisables dans le fichier `.gitlab-ci.yml` sont consultables sur <https://docs.gitlab.com/ee/ci/yaml/README.html>.

#### ğŸ Un exemple de fichier .gitlab-ci.yml, dÃ©crit en Python

Un exemple officiel en Python est disponible sur le repository Gitlab de Gitlab: <https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Python.gitlab-ci.yml>, nous allons le dÃ©crire briÃ©vement ci-aprÃ¨s:

```yaml
## Un exemple de fichier .gitlab-ci.yml

# Le pipeline va s'exÃ©cuter dans une image docker.
# En l'ocurrence, il s'agit de l'image python officielle
# la plus Ã  jour dans le dockerhub: https://hub.docker.com/r/library/python/tags/
image: python:latest

# Il est possible de dÃ©finir des variables d'environnement
# qui seront disponibles dans la suite du pipeline.
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

# DÃ©claration des stages du pipeline dans lesquels les steps s'insÃ¨reront
stages:
  - tests
  - build

# Une step nommÃ©e "test" est au stage nommÃ© "tests" 
test:
  stage: tests
  script:
    - python setup.py test
    - pip install tox flake8  # you can also use tox
    - tox -e py36,flake8

# Une step nommÃ© "packaging" est ajoutÃ©e au stage nommÃ© "build".
# Elle s'exÃ©cutera si l'Ã©tape "test" se termine avec succÃ¨s.
packaging:
  stage: build 
  dependencies: ["code-quality"]
  script:
    - python setup.py bdist_wheel
  # La direction "artifacts" permet de sauvegarder
  # des objets construits lors de l'exÃ©cution du pipeline.
  artifacts:
    paths:
      - dist/*.whl
```

## TP 3 (bonus)

```plaintext
âš ï¸ Si vous vous sentez en retard; laissez de cotÃ© ce bonus; 
Vous pourrez y revenir plus tard ğŸ“… ğŸ± ğŸ”®
```

1. Dans la step `python-tests`, faites en sorte que la commande `pytest` calcule la couverture de tests sur le module `gilded_rose` et produise la mesure de couverture en console.
   1. Vous pouvez utiliser [le package pytest-cov](https://pytest-cov.readthedocs.io/en/latest/readme.html#installation) pour y arriver.
2. Faites en sorte que la commande `pytest` calcule la couverture de tests sur le module `gilded_rose` et produise les mesures dans un rapport au format HTML.
   1. pytest-cov permet de gÃ©nÃ©rer des rapports, [consulter la documentation](https://pytest-cov.readthedocs.io/en/latest/reporting.html) pour voir comment faire Ã§a.
3. Changez la destination de production de ces rapports afin de les produire dans un dossier [reports/pytest/](../reports/pytest) Ã  la racine du repo.
4. Rendez les rapports du dossier `reports/` accessibles sous la forme d'artÃ©facts. 
   1. Un exemple d'utilisation de [la fonctionnalitÃ© d'artÃ©fact](https://docs.gitlab.com/ee/ci/pipelines/job_artifacts.html) est consultable dans l'extrait de fichier yml du TP prÃ©cÃ©dent (partie _ğŸ Exemple dÃ©crit en Python_)

**ğŸ Test de recette : les rapports sont disponibles dans gitlab-ci, via le bouton `Browse`, comme suit Ã  droite :**

![](./docs/exercice1-artefacts.png)

ğŸ‘‰ Le bouton `Browse` devrait vous permettre de consulter les rapports HTML que vous avez produit dans la step `python-tests` :

![](./docs/exercice1-artefacts-rapports-html.png)

## TP 4 : Mesure de la qualitÃ© du code (local)

```plaintext
ğŸ¯ Objectif : je veux obtenir du feedback sur la qualitÃ© du code sur commande.
```

ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ La qualitÃ© du code, c'est une notion subjective qui se dÃ©finit gÃ©nÃ©ralement en Ã©quipe.
ğŸ“Š Une fois qu'on l'a dÃ©fini collectivement, on peut dÃ©finir des indicateurs pour la mesurer.
ğŸ“¦ Certains package Python peuvent produire de tels indicateurs. 

**Par exemple :**

- [flake8](https://github.com/PyCQA/flake8) est un linter de code Python sur le style. Le nombre de warnings peut donner une indication de
la _compliance_ du code que l'on a produit avec les standards de style reconnus dans 
l'Ã©cosystÃ¨me Python.
ğŸ‘‰ On pourrait dÃ©finir que du code de qualitÃ©, c'est du code qui respecte ces standards et dont
le nombre de warning tend vers 0.

- [mypy](http://mypy-lang.org/) est un linter de code Python sur le _type hinting_. Le nombre de warnings levÃ©s donne une indication sur
le nombre de fonctions oÃ¹ 
  - le type hinting manque,
  - le type hinting est prÃ©sent mais erronÃ©,
  - etc ...
ğŸ‘‰ On pourrait dÃ©finir que du code de qualitÃ©, c'est du code oÃ¹ 
le type hinting est utilisÃ© systÃ©matiquement pour documenter le type de chaque argument 
et le type de la valeur de retour attendue. Donc du code oÃ¹ le nombre de warnings tend vers 0.

- [pytest-cov](https://github.com/pytest-dev/pytest-cov) est un plugin de pytest qui permet de profiter l'exÃ©cution des tests pour 
mesurer la couverture du code par les tests, c-a-d le ratio du nombre de lignes de code source traversÃ© par les tests sur le nombre de lignes de code total.
ğŸ‘‰ On pourrait dÃ©finir que du code de qualitÃ©, c'est du code oÃ¹ chaque ligne est testÃ©e, 
donc du code oÃ¹ le code coverage tend vers 100% (ou du moins dÃ©passe un seuil Ã©levÃ©, ex: 80%).

ğŸ‘‰ InsÃ©rez vos mÃ©triques favorites ici pour mesurer la qualitÃ© du code ou auditer du code ğŸ¤“
- Respect des ratios de la pyramide de tests,
- Respect de la clean architecture,
- Absence de code mort,
- Nombre de bugs,
- Seuil de complexitÃ© cyclomatique,
- MÃ©triques de sÃ©curitÃ© (nombre de failles/CVE dans le code ou les dÃ©pendances),
- MÃ©triques Accelerate (Lead time, MTTR, ...),
- ... 

ğŸ **Objectifs :**

```plaintext
> ğŸ•µï¸â€ Automatiser, c'est rendre automatique une action qui Ã©tait jusque-lÃ  manuelle ğŸ’ª
```

ğŸ‘‰ Commencez par essayer de mesurer la qualitÃ© du code en local ! 

Sur votre poste local, installez les outils suivant, mesurez la qualitÃ© de votre code en ligne de commande et affichez les rÃ©sultats de mesure en console avec :
- flake8
- mypy
- pytest-cov
- safety
- bandit

ğŸ¯ Mesurez les indicateurs suivant sur vos postes, en local :

- Nombre de warnings sur le style du code avec flake8,
- Nombre de warnings sur le type hinting avec flake8,
- Faites des analyses de sÃ©curitÃ© avec bandit et safety,
- Mesurer la complÃ©xitÃ© du code avec Pylama

Comme prÃ©cÃ©demment, prenez en note :
- les prÃ©-requis : les commandes ou paquets que vous avez dÃ» installer pour pouvoir lancer les tests,
- la commande que vous avez exÃ©cuter pour lancer les tests,
- le rÃ©sultat attendu : logs affichÃ©s en console, fichiers de rapport produits, ...

Cela nous servira pour reproduire cela dans notre pipeline de CI dans le prochain exercice.

Des exemples de sorties console sont disponibles dans le fichier [instructions/docs/analysis.txt](../docs/analysis.txt)

## TP 5 : Mesure de la qualitÃ© du code (CI)

```plaintext
ğŸ¯ Objectif : je veux obtenir du feedback sur la qualitÃ© du code automatiquement Ã  chaque push d'un commit.
```

Le pipeline doit permettre 
- d'Ã©xÃ©cuter les tests avec pytest, 
- puis si les tests sont verts; exÃ©cuter les Ã©tapes de mesure la qualitÃ© du code dans un stage `code-quality` comme ceci :

![](./docs/exercice2-code-analysis.png)

- âœ… Le stage `code-quality` sera vert si votre base de code respecte les standards de flake8, mypy, pytest-cov, safety, bandit
- ğŸ”´ Le stage `code-quality` sera rouge si l'un de ces outils d'analyse relÃ¨ve au moins 1 warning.

### Autoriser l'Ã©chec d'une step

Dans le TP suivant, nous allons ajouter une step supplÃ©mentaire au pipeline pour packager automatiquement l'application 
- si les tests sont verts,
- et si le code produit est "de qualitÃ© suffisante".

En l'Ã©tat, le code Python n'est pas "parfait" concernant les outils d'analyse que nous utilisons : il y a quelques warnings notables avec mypy et flake8 par exemple.

S'il est utile de savoir que ces warnings existent, et qu'il faudra les corriger, nous ne souhaitons pas pour autant que le pipeline de CI s'arrÃªte sur cette Ã©tape `code-quality`.

Pour permettre au pipeline de continuer, gitlab propose la fonctionnalitÃ© [allow-failure](https://docs.gitlab.com/ee/ci/yaml/#allow_failure).

ğŸ **Objectif : utilisez la fonctionnalitÃ© _allow failure_ sur le stage `code-quality` pour permettre au pipeline de ne pas s'arrÃªter mÃªme s'il Ã©choue sur celui-ci.**

**Rendu attendu** :

- âœ… Le stage `code-quality` sera vert si votre base de code respecte les standards de flake8, mypy, pytest-cov, safety, bandit
- âš ï¸ Le stage `code-quality` sera orange si l'un de ces outils d'analyse relÃ¨ve au moins 1 warning.

- ![](./docs/exercice2-allow-failure.png)

## TP 6 : Packager du code de qualitÃ© (local)

```plaintext
ğŸ¯ Objectif : je veux packager du code automatiquement.
```

Dans le dojo prÃ©cÃ©dent (rappelez-vous, c'Ã©tait il y a 1 mois ğŸ‘´), on avait packagÃ© notre application Python au format Wheel avec setuptools, et les commandes suivantes :

```shell
cd dojo-3;
pip install wheel;
python -m build --wheel; ï¸
```
ğŸ **Objectif 1 : Packagez l'application en local au format Wheel.**

ğŸœ Test de recette : un fichier au format .whl devrait Ãªtre apparu dans le dossier dist/ â—ï¸

ğŸ **Objectif 2 : Changez la version de l'application en 1.18.27 avant de la packager.**

ğŸœ Test de recette : un fichier gilded_rose-1.18.27-py3-none-any.whl devrait Ãªtre apparu dans le dossier dist/ â—ï¸

â„¹ï¸ Indice : un attribut `version` est dÃ©fini dans le fichier setup.cfg. Vous ne devez pas le modifier, mais Ã§a devrait vous mettre sur la piste.

ğŸ **Objectif 3 : Produisez une version unique de l'application automatiquement avec le package setuptools_scm.**

De la documentation sur ce package est disponible ici : <https://pypi.org/project/setuptools-scm/>

ğŸœ Test de recette : un fichier au nom ressemblant Ã  gilded_rose-0.1.dev5+g7b7b61f.d20220209-py3-none-any.whl devrait Ãªtre apparu dans le dossier dist/ â—ï¸

## TP 7 : Packager du code de qualitÃ© (en CI)

```plaintext
ğŸ¯ Objectif : je veux packager du code automatiquement Ã  chaque push d'un commit, si et seulement si les tests passent et la qualitÃ© du code est acceptable.
```

Le pipeline doit permettre 
- d'Ã©xÃ©cuter les tests avec pytest, 
- puis si les tests sont verts : exÃ©cuter les Ã©tapes de mesure la qualitÃ© du code dans un stage `code-quality`
- puis si les tests sont verts et la qualitÃ© OK : packager le code au format wheel avec une version unique.

ğŸ **Objectif 1 : Packagez l'application en local au format Wheel.**

ğŸœ Test de recette : une nouvelle step `package-wheel` un nouveau stage `build` dans le pipeline de CI doit permettre de produire un fichier au format .whl comme ceci : 

![](./docs/exercice3-build-stage.png)

ï¸ğŸ **Objectif 2 : Rendre le package accessible en artÃ©fact.**

ğŸœ Test de recette : la step `package-wheel` doit exposer le contenu du dossier dist/ en artÃ©fact afin de rendre le fichier .whl tÃ©lÃ©chargeable depuis l'interface web : 

![](./docs/exercice3-build-artifact.png)

ï¸ğŸ **Objectif 3 : Testez le wheel en local via les artÃ©facts**

ğŸœ Test de recette : le wheel tÃ©lÃ©chargÃ© via l'interface web et installÃ© localement avec pip fonctionne comme suit : 

![](./docs/exercice3-test-wheel-local.png)

ï¸ğŸ **Objectif 4 : Stocker le package dans une registry PyPI privÃ© sur gitlab**

Gitlab met Ã  disposition une registry PyPI par repository de code pour y entreposer les wheel produit automatiquement par la CI.

[En suivant la documentation](https://docs.gitlab.com/ee/user/packages/pypi_repository/#authenticate-with-a-ci-job-token) dans la partie `Authenticate with a CI job token`, 
modifiez la step `package-wheel` pour pousser le fichier .whl produit dans la registry PyPI du repository.

ğŸœ Test de recette : le wheel produit par la step `package-wheel` est disponible dans la registry PyPI du repo : 

![](./docs/exercice3-pypi.png)

ï¸ğŸ **Objectif 5 (bonus) : Testez votre wheel, dÃ©posÃ© dans la registry, aprÃ¨s l'avoir installÃ© en local avec pip install**
